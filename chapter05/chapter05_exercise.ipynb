{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Turn the code to get the data (from section 1. Get Data) into a Python script, such as `get_data.py`.\n",
    "\n",
    "* When you run the script using `python get_data.py` it should check if the data already exists and skip downloading if it does.\n",
    "* If the data download is successful, you should be able to access the `pizza_steak_sushi` images from the `data` directory."
   ],
   "id": "875180eaa2fdb274"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:41:02.972858Z",
     "start_time": "2024-11-07T12:41:02.966558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile get_data.py\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_sushi_steak\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"Directory: {image_path} exists.\")\n",
    "else:\n",
    "    print(f\"Directory: {image_path} no found, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(f\"Unzipping pizza, steak, sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "        print(\"Done.\")\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data.py\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Use [Python's `argparse` module](https://docs.python.org/3/library/argparse.html) to be able to send the `train.py` custom hyperparameter values for training procedures.\n",
    "* Add an argument flag for using a different:\n",
    "  * Training/testing directory\n",
    "  * Learning rate\n",
    "  * Batch size\n",
    "  * Number of epochs to train for\n",
    "  * Number of hidden units in the TinyVGG model\n",
    "    * Keep the default values for each of the above arguments as what they already are (as in notebook 05).\n",
    "* For example, you should be able to run something similar to the following line to train a TinyVGG model with a learning rate of 0.003 and a batch size of 64 for 20 epochs: `python train.py --learning_rate 0.003 batch_size 64 num_epochs 20`.\n",
    "* **Note:** Since `train.py` leverages the other scripts we created in section 05, such as, `model_builder.py`, `utils.py` and `engine.py`, you'll have to make sure they're available to use too. You can find these in the [`going_modular` folder on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular/going_modular). "
   ],
   "id": "bda9247a024a1d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:41:07.562074Z",
     "start_time": "2024-11-07T12:41:07.558002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile data_setup.py\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_dir: str,\n",
    "        test_dir: str,\n",
    "        transform: v2,\n",
    "        batch_size: int,\n",
    "        num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    \n",
    "    train_data = datasets.ImageFolder(\n",
    "        train_dir,\n",
    "        transform,\n",
    "    )\n",
    "    \n",
    "    test_data = datasets.ImageFolder(\n",
    "        test_dir,\n",
    "        transform\n",
    "    )\n",
    "    \n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ],
   "id": "69eaf29c9c3104c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_setup.py\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:41:09.517543Z",
     "start_time": "2024-11-07T12:41:09.512939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile engine.py\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        loss_func: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    correct_samples, total_samples = 0, 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        train_logits = model(X)\n",
    "        loss = loss_func(train_logits, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_probs = torch.softmax(train_logits, dim=1)\n",
    "        train_preds = torch.argmax(train_probs, dim=1)\n",
    "        correct_samples += (train_preds == y).sum().item()\n",
    "        total_samples += len(y)\n",
    "    \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = correct_samples / total_samples\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        loss_func: torch.nn.Module,\n",
    "        device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    correct_samples, total_samples = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            test_logits = model(X)\n",
    "            loss = loss_func(test_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            correct_samples += (y == test_preds).sum().item()\n",
    "            total_samples += len(y)\n",
    "        \n",
    "        test_loss = test_loss / len(dataloader)\n",
    "        test_acc = correct_samples / total_samples\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "        train_dataloader: torch.utils.data.DataLoader,\n",
    "        test_dataloader: torch.utils.data.DataLoader,\n",
    "        loss_func: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int,\n",
    "        device: torch.device\n",
    ") -> Dict[str, List]:\n",
    "    \n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_func, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_func, device)\n",
    "        \n",
    "        print(\n",
    "            f\"Epochs: {epoch + 1} |\"\n",
    "            f\"Train_loss: {train_loss:.4f} |\"\n",
    "            f\"Train_acc: {train_acc:.2f} |\"\n",
    "            f\"Test_loss: {test_loss:.4f} |\"\n",
    "            f\"Test_acc: {test_acc:.2f} |\"\n",
    "        )\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    return results"
   ],
   "id": "98bd20662d4fffa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting engine.py\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:41:13.767635Z",
     "start_time": "2024-11-07T12:41:13.763530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile model_builder.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_unit: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_shape, hidden_unit, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_unit, hidden_unit, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_unit, hidden_unit, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_unit, hidden_unit, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_unit * 13 * 13, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.classifier(self.block_2(self.block_1(x)))\n",
    "        return y"
   ],
   "id": "21a8b1381d41be67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_builder.py\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:41:20.763053Z",
     "start_time": "2024-11-07T12:41:20.759673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(\n",
    "        model: torch.nn.Module,\n",
    "        target_dir: str,\n",
    "        model_name: str\n",
    "):\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    \n",
    "    model_save_path = target_dir_path / model_name\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(model.state_dict(), f=model_save_path)"
   ],
   "id": "34dc87221b864fe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:49:39.776384Z",
     "start_time": "2024-11-07T12:49:39.771776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Get some hyperparameters.\")\n",
    "\n",
    "parser.add_argument(\"--num_epochs\", default=10, type=int, help=\"the number of epochs to train for\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"the number of samples per batch\")\n",
    "parser.add_argument(\"--hidden_units\", default=10, type=int, help=\"the number of hidden units in hidden layers\")\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float, help=\"learning rate to use for model\")\n",
    "parser.add_argument(\"--train_dir\", default=\"data/pizza_sushi_steak/train\", type=str, help=\"directory file path to training data in standard image classification format\")\n",
    "parser.add_argument(\"--test_dir\", default=\"data/pizza_sushi_steak/test\", type=str, help=\"directory file path to testing data in standard image classification format\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "LEARNING_RATE = args.learning_rate\n",
    "print(f\"[INFO] Training a model for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE} using {HIDDEN_UNITS} hidden units and a learning rate of {LEARNING_RATE}\")\n",
    "\n",
    "train_dir = args.train_dir\n",
    "test_dir = args.test_dir\n",
    "print(f\"[INFO] Training data file: {train_dir}\")\n",
    "print(f\"[INFO] Testing data file: {test_dir}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_transform = v2.Compose([\n",
    "    v2.Resize(size=(64, 64)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transform, BATCH_SIZE)\n",
    "\n",
    "    model = model_builder.TinyVGG(input_shape=3, hidden_unit=HIDDEN_UNITS, output_shape=len(class_names)).to(device)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    engine.train(model, train_dataloader, test_dataloader, loss_fn, optimizer, NUM_EPOCHS, device)\n",
    "    \n",
    "    utils.save_model(model, \"models\", \"05_going_modular_script_mode_tinyvgg_model.pth\")"
   ],
   "id": "2cc9845019000d03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:49:53.253619Z",
     "start_time": "2024-11-07T12:49:45.608078Z"
    }
   },
   "cell_type": "code",
   "source": "!python train.py --num_epochs 5 --batch_size 128 --hidden_units 128 --learning_rate 3e-4",
   "id": "2e69fabda85911d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training a model for 5 epochs with batch size 128 using 128 hidden units and a learning rate of 0.0003\n",
      "[INFO] Training data file: data/pizza_sushi_steak/train\n",
      "[INFO] Testing data file: data/pizza_sushi_steak/test\n",
      "Epochs: 1 |Train_loss: 1.1072 |Train_acc: 0.35 |Test_loss: 1.0988 |Test_acc: 0.33 |\n",
      "Epochs: 2 |Train_loss: 1.0923 |Train_acc: 0.35 |Test_loss: 1.0820 |Test_acc: 0.33 |\n",
      "Epochs: 3 |Train_loss: 1.0811 |Train_acc: 0.37 |Test_loss: 1.0669 |Test_acc: 0.41 |\n",
      "Epochs: 4 |Train_loss: 1.0603 |Train_acc: 0.48 |Test_loss: 1.0425 |Test_acc: 0.49 |\n",
      "Epochs: 5 |Train_loss: 1.0168 |Train_acc: 0.52 |Test_loss: 1.0178 |Test_acc: 0.44 |\n",
      "[INFO] Saving model to: models\\05_going_modular_script_mode_tinyvgg_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.06it/s]\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.23it/s]\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.33it/s]\n",
      " 80%|████████  | 4/5 [00:03<00:00,  1.39it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
